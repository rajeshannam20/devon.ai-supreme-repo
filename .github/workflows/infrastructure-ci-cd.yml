
name: Infrastructure CI/CD Pipeline

on:
  push:
    branches: [main, develop]
    paths:
      - 'terraform/**'
      - '.github/workflows/infrastructure-ci-cd.yml'
      - 'scripts/generate-tf-files.js'
      - 'policy/**'
      - 'tests/infrastructure/**'
  pull_request:
    branches: [main]
    paths:
      - 'terraform/**'
      - 'policy/**'
      - 'scripts/generate-tf-files.js'
      - 'tests/infrastructure/**'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target deployment environment'
        required: true
        default: 'production'
        type: choice
        options:
          - development
          - staging
          - production
      deployment_strategy:
        description: 'Deployment strategy'
        required: true
        default: 'canary'
        type: choice
        options:
          - standard
          - canary
          - blue-green

permissions:
  id-token: write
  contents: read
  pull-requests: write
  issues: write
  deployments: write
  security-events: write  
  packages: write

env:
  TF_VERSION: '1.5.7'
  TF_LOG: 'INFO'
  COMPLIANCE_STANDARDS: 'cis,gdpr,hipaa,pci'
  AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  ARM_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET_VALUE }}
  AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  AZURE_RESOURCE_GROUP: devonn-${{ github.event.inputs.environment || 'production' }}-rg
  CONTAINER_APP_NAME: devonn-ai
  CONTAINER_ENV_NAME: devonn-${{ github.event.inputs.environment || 'production' }}-env  
  AZURE_RESOURCE_GROUP_TFSTATE: devonn-tfstate-rg    

jobs:
  validate:
    name: Validate Terraform
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Format Check
        run: terraform fmt -recursive
        
      - name: Terraform Init
        run: terraform init -reconfigure
        
      - name: Terraform Validate
        run: terraform validate

      - name: Generate Documentation
        run: |
          # Install terraform-docs
          curl -sSLo ./terraform-docs.tar.gz https://github.com/terraform-docs/terraform-docs/releases/download/v0.19.0/terraform-docs-v0.19.0-linux-amd64.tar.gz
          
          # Check if the file is in gzip format
          if ! file terraform-docs.tar.gz | grep -q 'gzip compressed data'; then
            echo "Error: The downloaded file is not in gzip format."
            exit 1
          fi
          
          # Extract and install terraform-docs
          tar -xzf terraform-docs.tar.gz
          chmod +x terraform-docs
          sudo mv terraform-docs /usr/local/bin/terraform-docs
          
          # Generate markdown documentation
          terraform-docs markdown . > TERRAFORM.md
          
      - name: Upload Documentation
        uses: actions/upload-artifact@v4
        with:
          name: terraform-docs
          path: TERRAFORM.md
  
  security-scan:
    name: Security Scan
    needs: validate
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
          
      - name: Run tfsec
        uses: aquasecurity/tfsec-action@v1.0.0
        with:
          soft_fail: true
          format: sarif
          additional_args: -O tfsec.sarif
      
      - name: Run checkov
        uses: bridgecrewio/checkov-action@master
        with:
          directory: ./src/data/manifest/terraform/azure
          framework: terraform
          soft_fail: true
          output_format: sarif

      - name: Upload tfsec results
        uses: github/codeql-action/upload-sarif@v3
        if: success() || failure()
        with:
          token: ${{ secrets.GITHUB_TOKEN }}        
          sarif_file: ./tfsec.sarif
          category: tfsec

          
      - name: Upload checkov results
        uses: github/codeql-action/upload-sarif@v3
        if: success() || failure()
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          sarif_file: ./results.sarif
          category: checkov
      
      - name: Install OPA
        run: |
          curl -L -o opa https://openpolicyagent.org/downloads/v0.43.0/opa_linux_amd64_static
          chmod +x opa
          sudo mv opa /usr/local/bin/
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Install dependencies
        run: |
          npm install --save-dev @types/node --legacy-peer-deps

      - name: Generate Terraform files from manifests
        run: |
          npx ts-node scripts/generate-tf-files.cjs

      - name: List generated files
        working-directory: ./src/data/manifest/terraform
        run: |
          ls -lh .          

      # - name: Generate dynamic tfvars override based on existing IAM roles
      #   shell: bash
      #   env:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #   run: |
      #     echo "# Dynamically generated tfvars for security scan" > ./src/data/manifest/terraform/tfvars_override.tfvars
      
      #     # Backup role
      #     if aws iam get-role --role-name devonn-backup-role-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_backup_role = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_backup_role = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi
      
      #     # VPC Flow Log role
      #     if aws iam get-role --role-name devonn-vpc-flow-log-role-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_vpc_flow_log_role = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_vpc_flow_log_role = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi
      
      #     # X-Ray role
      #     if aws iam get-role --role-name devonn-xray-role-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_xray_role = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_xray_role = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi

      #     if aws backup get-backup-vault --backup-vault-name devonn-rds-backup-vault-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_backup_vault = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_backup_vault = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi          

      #     if aws cloudtrail describe-trails --trail-name-list devonn-cloudtrail-prod --region $AWS_REGION | grep devonn-cloudtrail-prod >/dev/null 2>&1; then
      #       echo 'create_cloudtrail = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_cloudtrail = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi          

      - name: Check and Release Terraform State Lock (AWS)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          LOCK_ID="devonn/terraform.tfstate"
          TABLE_NAME="terraform-locks"
          AWS_REGION="us-west-2"
          echo "Checking for Terraform state lock in DynamoDB (region: $AWS_REGION)..."
          LOCK_EXISTS=$(aws dynamodb get-item --table-name "$TABLE_NAME" --key "{\"LockID\": {\"S\": \"$LOCK_ID\"}}" --region "$AWS_REGION" --query 'Item.LockID.S' --output text 2>&1)
          EXIT_CODE=$?
          echo "GetItem output: $LOCK_EXISTS"
          if [ $EXIT_CODE -ne 0 ]; then
            echo "Error running get-item. Check permissions, region, and table existence."
            exit $EXIT_CODE
          fi
          if [ "$LOCK_EXISTS" = "$LOCK_ID" ]; then
            echo "Lock found, attempting to release..."
            DELETE_OUTPUT=$(aws dynamodb delete-item --table-name "$TABLE_NAME" --key "{\"LockID\": {\"S\": \"$LOCK_ID\"}}" --region "$AWS_REGION" 2>&1)
            DELETE_EXIT=$?
            echo "DeleteItem output: $DELETE_OUTPUT"
            if [ $DELETE_EXIT -ne 0 ]; then
              echo "Error releasing lock. Check permissions."
              exit $DELETE_EXIT
            fi
            echo "Lock released."
          else
            echo "No lock found, continuing workflow."
          fi


      - name: Generate Terraform Plan for Policy Evaluation
        working-directory: ./src/data/manifest/terraform
        env:
          TF_VAR_aws_access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_db_password: ${{ secrets.RDS_DB_PASSWORD }}
        run: |
          terraform init -reconfigure
          terraform plan -out=tfplan -lock=false
          if [ $? -ne 0 ]; then
            echo "Terraform plan failed"
            exit 1
          fi
          terraform show -json tfplan > tfplan.json
      
      - name: Find Terraform Plan File
        run: |
          echo "Searching for the tfplan file..."
          find . -name "tfplan" 
          echo "Listing all files in ./src/data/manifest/terraform..."
          ls -l ./src/data/manifest/terraform
          echo "Listing all files in ./src/data/manifest/terraform..."
          cat ./src/data/manifest/terraform/tfplan.json

      - name: Run OPA Policy Checks
        run: |
          mkdir -p policy-results
          
          # Run RDS policy checks
          opa eval --format pretty --data policy/aws_rds_policy.rego --input ./src/data/manifest/terraform/tfplan.json "data.terraform.aws.rds" > policy-results/rds-policy-results.txt
          
          echo "OPA Policy Check Results:"
          cat policy-results/rds-policy-results.txt
          
          # Check for deny results
          if grep -q "deny" policy-results/rds-policy-results.txt; then
            echo "‚ö†Ô∏è Policy violations detected in RDS configuration"
            # Exit with failure code in PR, but continue in workflow dispatch
            if [ "${{ github.event_name }}" == "pull_request" ]; then
              exit 1
            fi
          fi

      
      - name: Upload Policy Check Results
        uses: actions/upload-artifact@v4
        with:
          name: policy-check-results
          path: policy-results/
  
  terraform-plan:
    name: Terraform Plan
    needs: [validate, security-scan]
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || (github.ref == 'refs/heads/main' && 'production' || 'development') }}
    outputs:
      plan_output: ${{ steps.plan.outputs.stdout }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
          
      - name: Install dependencies
        run: |
          npm install --save-dev @types/node --legacy-peer-deps

      - name: Generate Terraform files from manifests
        run: |
          npx ts-node scripts/generate-tf-files.cjs

      - name: Terraform Init
        working-directory: ./src/data/manifest/terraform    
        id: init
        run: terraform init -reconfigure

      - name: List directory before upload
        working-directory: ./src/data/manifest/terraform
        run: ls -alh

      - name: Debug lock file presence
        run: ls -alh src/data/manifest/terraform/.terraform.lock.hcl

      - name: Upload Terraform Lock File
        uses: actions/upload-artifact@v4
        with:
          name: terraform-lock
          path: src/data/manifest/terraform/.terraform.lock.hcl
          include-hidden-files: true

      - name: Upload Terraform Config Files
        uses: actions/upload-artifact@v4
        with:
          name: terraform-config
          path: ./src/data/manifest/terraform/*.tf      

      # - name: Generate dynamic tfvars override based on existing IAM roles
      #   shell: bash
      #   env:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_REGION: ${{ secrets.AWS_REGION }}
      #     TF_VAR_aws_envoy_crt: ${{ secrets.AWS_ENVOY_CRT }}
      #     TF_VAR_aws_envoy_key: ${{ secrets.AWS_ENVOY_KEY }}
      #   run: |
      #     echo "# Dynamically generated tfvars" > ./src/data/manifest/terraform/tfvars_override.tfvars
      
      #     # Check for backup role existence
      #     if aws iam get-role --role-name devonn-backup-role-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_backup_role = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_backup_role = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi
      
      #     # Repeat similar checks for other roles (replace as needed)
      #     if aws iam get-role --role-name devonn-vpc-flow-log-role-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_vpc_flow_log_role = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_vpc_flow_log_role = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi
      
      #     if aws iam get-role --role-name devonn-xray-role-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_xray_role = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_xray_role = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi   

      #     if aws backup get-backup-vault --backup-vault-name devonn-rds-backup-vault-prod --region $AWS_REGION >/dev/null 2>&1; then
      #       echo 'create_backup_vault = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_backup_vault = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi

      #     if aws cloudtrail describe-trails --trail-name-list devonn-cloudtrail-prod --region $AWS_REGION | grep devonn-cloudtrail-prod >/dev/null 2>&1; then
      #       echo 'create_cloudtrail = false' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     else
      #       echo 'create_cloudtrail = true' >> ./src/data/manifest/terraform/tfvars_override.tfvars
      #     fi          

      # - name: Upload tfvars override
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: tfvars-override
      #     path: ./src/data/manifest/terraform/tfvars_override.tfvars       

      - name: Terraform Plan
        working-directory: ./src/data/manifest/terraform     
        env:
          TF_VAR_aws_access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_db_password: ${{ secrets.RDS_DB_PASSWORD }}
        id: plan
        run: |
          terraform plan -out=tfplan -lock=false
          terraform show -json tfplan > tfplan.json

      - name: List Terraform directory after plan
        working-directory: ./src/data/manifest/terraform
        run: |
          ls -alh .   

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan
          path: |
            ./src/data/manifest/terraform/tfplan
            ./src/data/manifest/terraform/tfplan.json          
          
      - name: Comment Plan on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const planOutput = fs.readFileSync('tfplan.json', 'utf8');
            const planJson = JSON.parse(planOutput);
            
            // Extract resource changes for a summary
            const resourceChanges = planJson.resource_changes || [];
            const addCount = resourceChanges.filter(r => r.change.actions.includes('create')).length;
            const changeCount = resourceChanges.filter(r => r.change.actions.includes('update')).length;
            const deleteCount = resourceChanges.filter(r => r.change.actions.includes('delete')).length;
            
            const output = `#### Terraform Plan Summary üìù
            
            * Resources to add: ${addCount}
            * Resources to change: ${changeCount}
            * Resources to destroy: ${deleteCount}
            
            <details><summary>Show Details</summary>
            
            \`\`\`
            ${process.env.PLAN_OUTPUT}
            \`\`\`
            
            </details>
            
            *Pushed by: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })
        env:
          PLAN_OUTPUT: ${{ steps.plan.outputs.stdout }}
  
  approval:
    name: Deployment Approval
    needs: terraform-plan
    if: github.event.inputs.environment == 'production' || github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production-approval
    steps:
      - name: Manual approval step
        run: echo "Deployment to production has been approved"
  
  apply:
    name: Terraform Apply
    needs: [terraform-plan, approval]
    if: |
      success() && 
      (github.event_name == 'workflow_dispatch' || github.ref == 'refs/heads/main')
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    concurrency: 
      group: ${{ github.workflow }}-${{ github.event.inputs.environment || 'production' }}
      cancel-in-progress: false
    outputs:
      rds_endpoint: ${{ steps.outputs.outputs.rds_endpoint }}
      cluster_name: ${{ steps.outputs.outputs.cluster_name }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
          
      - name: Download Terraform Lock File
        uses: actions/download-artifact@v4
        with:
          name: terraform-lock
          path: ./src/data/manifest/terraform

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan
          path: ./src/data/manifest/terraform

      - name: Download Terraform Config Files
        uses: actions/download-artifact@v4
        with:
          name: terraform-config
          path: ./src/data/manifest/terraform          

      - name: Debug Terraform Files
        run: ls -alh ./src/data/manifest/terraform

      - name: Terraform Init
        working-directory: ./src/data/manifest/terraform
        run: terraform init -reconfigure  

      # - name: Terraform Destroy to clean existing resources
      #   working-directory: ./src/data/manifest/terraform      
      #   env:
      #     TF_VAR_aws_access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     TF_VAR_aws_secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
      #     TF_VAR_db_password: ${{ secrets.RDS_DB_PASSWORD }}        
      #   run: |
      #     echo "Running terraform destroy to cleanup existing conflicting resources..."
      #     terraform destroy -auto-approve || echo "Destroy failed or no resources to destroy, continuing..."        
        
      - name: Create deployment status
        id: deployment
        uses: bobheadxi/deployments@v1
        with:
          step: start
          token: ${{ secrets.GITHUB_TOKEN }}
          env: ${{ github.event.inputs.environment || 'production' }}
          ref: ${{ github.ref }}
      
      # - name: Terraform Import Existing Resources
        # working-directory: ./src/data/manifest/terraform
        # env:
        #   TF_VAR_aws_access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
        #   TF_VAR_aws_secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        #   TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
        #   TF_VAR_db_password: ${{ secrets.RDS_DB_PASSWORD }}
        #   TF_VAR_aws_envoy_crt: ${{ secrets.AWS_ENVOY_CRT }}
        #   TF_VAR_aws_envoy_key: ${{ secrets.AWS_ENVOY_KEY }}
        # run: |
        #     VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=devonn-vpc-*" --query 'Vpcs[0].VpcId' --output text)
        #     PRIVATE_SUBNET_IDS=( $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" "Name=tag:Type,Values=private" --query 'Subnets[].SubnetId' --output text) )
        #     PUBLIC_SUBNET_IDS=( $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" "Name=tag:Type,Values=public" --query 'Subnets[].SubnetId' --output text) )
        #     EIP_ALLOC_IDS=( $(aws ec2 describe-addresses --query 'Addresses[].AllocationId' --output text) )
        #     KMS_ALIAS_ARN=$(aws kms list-aliases --query 'Aliases[?AliasName==`alias/eks/devonn-eks-prod`].AliasArn' --output text)

        #     # Helper function for import with AWS existence check and fallback create
        #     import_or_create() {
        #       RESOURCE="$1"
        #       ID="$2"
        #       EXISTS_CMD="$3"
        #       CREATE_CMD="$4"
        #       echo "Importing $RESOURCE with ID $ID..."
        #       terraform import $RESOURCE "$ID" && return
        #       echo "Import failed, checking if resource exists in AWS..."
        #       if eval "$EXISTS_CMD"; then
        #         echo "Resource exists in AWS, skipping create."
        #       else
        #         echo "Resource does not exist, attempting to create $RESOURCE..."
        #         eval "$CREATE_CMD"
        #       fi
        #     }

        #     # Import with AWS existence check and fallback create
        #     import_or_create aws_db_parameter_group.postgres_production[0] devonn-postgres-params-prod "aws rds describe-db-parameter-groups --db-parameter-group-name devonn-postgres-params-prod | grep devonn-postgres-params-prod" "terraform apply -target=aws_db_parameter_group.postgres_production[0] -auto-approve"
        #     import_or_create aws_backup_vault.rds_backup_vault[0] devonn-rds-backup-vault-prod "aws backup describe-backup-vault --backup-vault-name devonn-rds-backup-vault-prod | grep devonn-rds-backup-vault-prod" "terraform apply -target=aws_backup_vault.rds_backup_vault[0] -auto-approve"
        #     import_or_create aws_iam_role.backup_role[0] devonn-backup-role-prod "aws iam get-role --role-name devonn-backup-role-prod | grep devonn-backup-role-prod" "terraform apply -target=aws_iam_role.backup_role[0] -auto-approve"
        #     import_or_create aws_cloudtrail.devonn_cloudtrail[0] devonn-cloudtrail-prod "aws cloudtrail describe-trails --trail-name-list devonn-cloudtrail-prod | grep devonn-cloudtrail-prod" "terraform apply -target=aws_cloudtrail.devonn_cloudtrail[0] -auto-approve"
        #     import_or_create aws_cloudwatch_log_group.flow_log_group[0] /aws/vpc-flow-logs/devonn-vpc-prod "aws logs describe-log-groups --log-group-name-prefix /aws/vpc-flow-logs/devonn-vpc-prod | grep /aws/vpc-flow-logs/devonn-vpc-prod" "terraform apply -target=aws_cloudwatch_log_group.flow_log_group[0] -auto-approve"
        #     import_or_create aws_iam_role.vpc_flow_log_role[0] devonn-vpc-flow-log-role-prod "aws iam get-role --role-name devonn-vpc-flow-log-role-prod | grep devonn-vpc-flow-log-role-prod" "terraform apply -target=aws_iam_role.vpc_flow_log_role[0] -auto-approve"
        #     import_or_create aws_iam_role.config_role[0] devonn-config-role-prod "aws iam get-role --role-name devonn-config-role-prod | grep devonn-config-role-prod" "terraform apply -target=aws_iam_role.config_role[0] -auto-approve"
        #     import_or_create aws_appmesh_mesh.devonn_mesh devonn-mesh-prod "aws appmesh describe-mesh --mesh-name devonn-mesh-prod | grep devonn-mesh-prod" "terraform apply -target=aws_appmesh_mesh.devonn_mesh -auto-approve"
        #     import_or_create aws_iam_role.xray_role[0] devonn-xray-role-prod "aws iam get-role --role-name devonn-xray-role-prod | grep devonn-xray-role-prod" "terraform apply -target=aws_iam_role.xray_role[0] -auto-approve"
        #     import_or_create aws_kms_alias.this["cluster"] "$KMS_ALIAS_ARN" "aws kms list-aliases --query 'Aliases[?AliasName==`alias/eks/devonn-eks-prod`].AliasArn' | grep $KMS_ALIAS_ARN || true" "terraform apply -target=aws_kms_alias.this[\"cluster\"] -auto-approve || true"
        #     import_or_create module.eks.module.kms.aws_kms_alias.this["cluster"] "$KMS_ALIAS_ARN" "aws kms list-aliases --query 'Aliases[?AliasName==`alias/eks/devonn-eks-prod`].AliasArn' | grep $KMS_ALIAS_ARN || true" "terraform apply -target=module.eks.module.kms.aws_kms_alias.this[\"cluster\"] -auto-approve || true"
        #     import_or_create aws_cloudwatch_log_group.this[0] /aws/eks/devonn-eks-prod/cluster "aws logs describe-log-groups --log-group-name-prefix /aws/eks/devonn-eks-prod/cluster | grep /aws/eks/devonn-eks-prod/cluster" "terraform apply -target=aws_cloudwatch_log_group.this[0] -auto-approve"

        #     import_or_create module.vpc.aws_subnet.private[0] ${PRIVATE_SUBNET_IDS[0]} "aws ec2 describe-subnets --subnet-ids ${PRIVATE_SUBNET_IDS[0]} | grep ${PRIVATE_SUBNET_IDS[0]}" "terraform apply -target=module.vpc.aws_subnet.private[0] -auto-approve"
        #     import_or_create module.vpc.aws_subnet.private[1] ${PRIVATE_SUBNET_IDS[1]} "aws ec2 describe-subnets --subnet-ids ${PRIVATE_SUBNET_IDS[1]} | grep ${PRIVATE_SUBNET_IDS[1]}" "terraform apply -target=module.vpc.aws_subnet.private[1] -auto-approve"
        #     import_or_create module.vpc.aws_subnet.public[0] ${PUBLIC_SUBNET_IDS[0]} "aws ec2 describe-subnets --subnet-ids ${PUBLIC_SUBNET_IDS[0]} | grep ${PUBLIC_SUBNET_IDS[0]}" "terraform apply -target=module.vpc.aws_subnet.public[0] -auto-approve"
        #     import_or_create module.vpc.aws_subnet.public[1] ${PUBLIC_SUBNET_IDS[1]} "aws ec2 describe-subnets --subnet-ids ${PUBLIC_SUBNET_IDS[1]} | grep ${PUBLIC_SUBNET_IDS[1]}" "terraform apply -target=module.vpc.aws_subnet.public[1] -auto-approve"

        #     import_or_create module.vpc.aws_eip.nat[0] ${EIP_ALLOC_IDS[0]} "aws ec2 describe-addresses --allocation-ids ${EIP_ALLOC_IDS[0]} | grep ${EIP_ALLOC_IDS[0]}" "terraform apply -target=module.vpc.aws_eip.nat[0] -auto-approve"

        #     import_or_create module.rds.module.db_instance.aws_iam_role.enhanced_monitoring[0] devonn-rds-monitoring-role-prod "aws iam get-role --role-name devonn-rds-monitoring-role-prod | grep devonn-rds-monitoring-role-prod" "terraform apply -target=module.rds.module.db_instance.aws_iam_role.enhanced_monitoring[0] -auto-approve"

        #     import_or_create module.eks.aws_cloudwatch_log_group.this[0] /aws/eks/devonn-eks-prod/cluster "aws logs describe-log-groups --log-group-name-prefix /aws/eks/devonn-eks-prod/cluster | grep /aws/eks/devonn-eks-prod/cluster" "terraform apply -target=module.eks.aws_cloudwatch_log_group.this[0] -auto-approve"

        #     # Add missing imports for VPC, log groups, etc. with fallback
        #     import_or_create module.vpc.aws_vpc.this[0] $VPC_ID "aws ec2 describe-vpcs --vpc-ids $VPC_ID | grep $VPC_ID" "terraform apply -target=module.vpc.aws_vpc.this[0] -auto-approve"

      - name: Terraform Refresh
        working-directory: ./src/data/manifest/terraform
        env:
          TF_VAR_aws_access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_db_password: ${{ secrets.RDS_DB_PASSWORD }}     
        run: terraform refresh -lock=false

      - name: Terraform Plan (fresh)
        working-directory: ./src/data/manifest/terraform
        env:
          TF_VAR_aws_access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
          TF_VAR_aws_secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_db_password: ${{ secrets.RDS_DB_PASSWORD }}       
        run: terraform plan -out=tfplan -lock=false

      # - name: Download tfvars override
      #   uses: actions/download-artifact@v4
      #   with:
      #     name: tfvars-override
      #     path: ./src/data/manifest/terraform        

      # Use selected deployment strategy
      - name: Canary Deployment
        if: github.event.inputs.deployment_strategy == 'canary' || github.event.inputs.deployment_strategy == null
        working-directory: ./src/data/manifest/terraform
        run: |
          echo "Implementing canary deployment strategy..."
          terraform apply -auto-approve -lock=false ./tfplan
          
      
      - name: Blue-Green Deployment
        if: github.event.inputs.deployment_strategy == 'blue-green'
        run: |
          echo "Implementing blue-green deployment strategy..."
          # Clone current environment as blue environment
          export BLUE_ENV="${{ github.event.inputs.environment || 'production' }}-blue"
          
          # Setup blue environment
          terraform workspace new $BLUE_ENV || terraform workspace select $BLUE_ENV
          
          # Apply configuration directly with variables passed through -var
          terraform apply -var "environment=${{ github.event.inputs.environment || 'production' }}" -var "aws_region=${{ secrets.AWS_REGION }}" -auto-approve -lock=false ./tfplan
          
          echo "Blue environment deployed, waiting for validation..."
          sleep 180
          
          # Switch traffic to blue
          echo "Switching traffic to new environment..."
          # In a real scenario, this would update a load balancer or DNS
          
          echo "Deployment complete, old environment will be decommissioned later"
      
      - name: Standard Deployment
        if: github.event.inputs.deployment_strategy == 'standard'
        run: |
          echo "Implementing standard deployment strategy..."
          terraform apply -auto-approve -lock=false ./tfplan
      
      - name: Extract Terraform Outputs
        id: outputs
        if: success()
        working-directory: ./src/data/manifest/terraform
        run: |
          echo "Fetching Terraform output after apply..."
          export TF_LOG=ERROR
          
          # Capture the raw output for eks_cluster_name
          RAW_CLUSTER_NAME=$(terraform output -raw eks_cluster_name 2>&1)
          
          # Clean and save eks_cluster_name
          CLUSTER_NAME=$(echo "$RAW_CLUSTER_NAME" | sed 's/::debug::.*//g' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
          echo "cluster_name=${CLUSTER_NAME}" >> $GITHUB_ENV
          
          # Capture the raw output for rds_endpoint
          RAW_RDS_ENDPOINT=$(terraform output -raw rds_endpoint 2>&1)
          
          # Clean and save rds_endpoint
          RDS_ENDPOINT=$(echo "$RAW_RDS_ENDPOINT" | sed 's/::debug::.*//g' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
          echo "rds_endpoint=${RDS_ENDPOINT}" >> $GITHUB_ENV

          # Debug the values to ensure correctness
          echo "cluster_name=$CLUSTER_NAME"
          echo "rds_endpoint=$RDS_ENDPOINT"

      - name: Debug environment variables
        run: |
          echo "cluster_name: ${{ env.cluster_name }}"
          echo "rds_endpoint: ${{ env.rds_endpoint }}"
          echo "DEBUG - cluster_name: ${{ needs.apply.outputs.cluster_name }}"
          echo "DEBUG - rds_endpoint: ${{ needs.apply.outputs.rds_endpoint }}"

      - name: Update deployment status
        if: always()
        uses: bobheadxi/deployments@v1
        with:
          step: finish
          token: ${{ secrets.GITHUB_TOKEN }}
          status: ${{ job.status }}
          deployment_id: ${{ steps.deployment.outputs.deployment_id }}
          env: ${{ github.event.inputs.environment || 'production' }}
  
  post-deploy-validation:
    name: Post-Deployment Validation
    needs: apply
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'
      
      - name: Debug environment variables
        run: |
          echo "DEBUG - cluster_name: ${{ env.cluster_name }}"
          echo "DEBUG - rds_endpoint: ${{ env.rds_endpoint }}"

      - name: Run Infrastructure Tests
        env:
          eks_cluster_name: ${{ needs.apply.outputs.cluster_name }}
          rds_endpoint: ${{ needs.apply.outputs.rds_endpoint }}
        run: |
          cd tests/infrastructure
          go mod init infrastructure-tests
          go mod tidy
          go test -v -timeout 30m -tags=integration
      
      - name: Check Database Connectivity
        env:
          DB_ENDPOINT: ${{ needs.apply.outputs.rds_endpoint }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          # Install PostgreSQL client
          sudo apt-get update
          sudo apt-get install -y postgresql-client
          
          # Test connection (password would be securely retrieved from secrets manager in real scenario)
          PGPASSWORD=${{ secrets.DB_PASSWORD }} psql -h $DB_ENDPOINT -U ${{ secrets.DB_USERNAME }} -d ${{ secrets.DB_NAME }} -c "SELECT 1 AS connectivity_test;"
      
      - name: Check EKS Cluster Health
        env:
          CLUSTER_NAME: ${{ env.cluster_name }}
        run: |
          # Configure kubectl
          aws eks update-kubeconfig --name $CLUSTER_NAME --region us-west-2
          
          # Check node status
          kubectl get nodes
          
          # Check pod status
          kubectl get pods --all-namespaces
          
          # Check service mesh status
          kubectl get meshes -A
      
      - name: Load Testing
        run: |
          # Install k6
          sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
          
          # Run basic load test
          k6 run load-tests/basic-load.js --summary-export=load-test-results.json
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        with:
          name: validation-results
          path: |
            load-test-results.json
      
      - name: Send Deployment Notification
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "‚úÖ Infrastructure deployment to ${{ github.event.inputs.environment || 'production' }} complete! All validation checks passed."
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

  compliance-check:
    name: Compliance Validation
    needs: [apply]
    if: success()
    runs-on: ubuntu-latest
    env:
      COMPLIANCE_STANDARDS: "cis_1.5_aws,iso27001_2022_aws,pci_3.2.1_aws"
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Install Prowler
        run: |
          pip install prowler
      
      - name: Run General Compliance Checks
        run: |
          prowler aws --compliance cis_1.5_aws --compliance iso27001_2022_aws --compliance pci_3.2.1_aws --output-formats html --output-filename compliance-report || true
  
      # Run compliance checks focusing on specific services (RDS and EKS)
      - name: Run Service-Specific Compliance Checks
        run: |
          prowler aws --services rds eks --output-formats html --output-filename compliance-report-services || true
      
      - name: Upload Compliance Report
        uses: actions/upload-artifact@v4
        with:
          name: compliance-reports
          path: output/compliance-report*
      
      - name: Generate Compliance Dashboard
        run: |
          # Parse compliance report to JSON for dashboard
          pip install beautifulsoup4
          python - <<EOF
          import json
          import re
          from bs4 import BeautifulSoup
          
          # Read HTML report
          with open('./output/compliance-report.html', 'r') as f:
              html = f.read()
          
          soup = BeautifulSoup(html, 'html.parser')
          
          # Extract compliance metrics
          passed = len(soup.find_all(class_='prowler-pass'))
          failed = len(soup.find_all(class_='prowler-fail'))
          total = passed + failed
          compliance_rate = (passed / total) * 100 if total > 0 else 0
          
          # Generate dashboard data
          dashboard = {
              "timestamp": "${{ github.event.repository.updated_at }}",
              "environment": "${{ github.event.inputs.environment || 'production' }}",
              "compliance_rate": compliance_rate,
              "passed": passed,
              "failed": failed,
              "standards": "${{ env.COMPLIANCE_STANDARDS }}".split(','),
              "report_url": "artifacts/compliance-reports/compliance-report.html"
          }
          
          # Save dashboard data
          with open('compliance-dashboard.json', 'w') as f:
              json.dump(dashboard, f)
          EOF
      
      - name: Upload Compliance Dashboard
        uses: actions/upload-artifact@v4
        with:
          name: compliance-dashboard
          path: compliance-dashboard.json
      
      - name: Send Compliance Report
        if: always()
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "üìä Compliance Report for ${{ github.event.inputs.environment || 'production' }} is available.",
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "Infrastructure Compliance Report"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "Environment: *${{ github.event.inputs.environment || 'production' }}*\nCompliance Standards: *${{ env.COMPLIANCE_STANDARDS }}*"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "View detailed reports in GitHub Actions artifacts."
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

  rollback:
    name: Rollback on Failure
    needs: [apply, post-deploy-validation]
    if: failure() && (github.event.inputs.environment == 'production' || github.ref == 'refs/heads/main')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Init
        run: terraform init
        
      - name: Rollback to Last Known Good State
        run: |
          echo "Deployment failed! Rolling back to last known good state..."
          terraform workspace select ${{ github.event.inputs.environment || 'production' }}
          terraform plan -refresh-only -var-file=environments/${{ github.event.inputs.environment || 'production' }}.tfvars -out=rollback.tfplan
          terraform apply rollback.tfplan
      
      - name: Send Rollback Notification
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "‚ö†Ô∏è ALERT: Infrastructure deployment to ${{ github.event.inputs.environment || 'production' }} failed! Rollback initiated.",
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "‚ö†Ô∏è DEPLOYMENT FAILURE - ROLLBACK INITIATED",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Environment:* ${{ github.event.inputs.environment || 'production' }}\n*Initiated by:* ${{ github.actor }}\n*Commit:* ${{ github.sha }}"
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "The system has been automatically rolled back to the last known good state. Please investigate the issue."
                  }
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "View Workflow Run",
                        "emoji": true
                      },
                      "url": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

  cleanup:
    name: Cleanup Resources
    needs: [post-deploy-validation, compliance-check]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Remove Temporary Files
        run: |
          echo "Cleaning up temporary files and resources..."
          # This would remove any temporary resources created during testing
          # Like test databases, test pods, etc.
